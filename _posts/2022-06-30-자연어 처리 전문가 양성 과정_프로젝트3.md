---
layout: post
title:  "자연어 처리 전문가 양성 과정_프로젝트3"
date:   2022-06-30
author: 이준엽
categories: 자연어처리전문가양성과정
tags:	자연어처리
cover:  "/assets/instacode.png"
---

[프로젝트3 - Github](https://github.com/Makeitshort/Profile/tree/main/Goorm%20Project/goorm_project3)

프로젝트3은 나만의 영-한 번역기를 만드는 것이었습니다. 저는 파파고, 구글 번역기 등은 팝송의 영어 가사를 넣고 번역을 시키면 직역으로 번역이 되어서 팝송의 느낌을 그대로 느끼기 어렵다는 것에서 착안해 초월번역을 할 수 있는 가사 번역기를 만들자고 제안했습니다.

우선은 데이터 셋을 만들었습니다. K팝이 해외에서 인기인 덕에 해외 팬들이 직접 한국어를 영어로 번역한 사이트가 있었습니다. 뿐만 아니라 국내 팝송 팬들이 직접 한글로 번역한 사이트도 있었습니다. 이런 여러 가사사이트들을 selinium을 사용해 크롤링했습니다. 총 약 26000개 수집했습니다.

전처리를 통해 크롤링한 데이터를 정제하고, 잘못된 번역은 없는지, 정제에서 잘못된 데이터는 없는지 일일이 확인했습니다. 생각보다 많은 데이터들이 전처리에서 사라져서 저희 팀은 데이터를 늘리고, 노이즈를 줄이기 위한 방법으로 back translation기법을 사용했습니다. Bert - KoGPT2, KoGPT2 - Bert로 만든 가사 번역기로 팝송, 국내가요를 back translation시켜 데이터를 총 24000개로 늘릴 수 있었습니다.

하지만 Bert - KoGPT2, KoGPT2 - Bert의 가사 번역기가 높은 성능을 보이지 않고, 번역도 생각보다는 떨어진다고 판단됐습니다. 허깅페이스를 이용해 어떤 모델이 좋을지 조사를 했습니다. 저희 팀은 Facebook의 m2m100을 사용하기로 했습니다. m2m100은 중간 언어로 영어를 통하지 않고 100개 언어를 직접 번역할 수 있고, 고품질의 양방향 번역데이터가 있다는 점이 강점입니다. 저희 팀의 태스크가 유연성이 있는 번역이었기 때문에 m2m100을 채택하는 것이 좋다고 생각했습니다.

저희는 구글코랩을 사용했기 때문에 한정된 GPU자원으로 모델을 돌려야해서 batch size나 epoch에 한계가 있었습니다. 때문에 최적의 learning-rat, batch size, epoch를 찾기 위해 시간이 오래 걸렸고 한계가 있지만, 저희가 찾은 최적의 수치들을 적용했습니다.

후처리도 했습니다. 출력에서 효율성을 높이기 위해 beam기법을 사용했고, 출력에서 원하는 번역을 제외하고도 추가되는 출력이 있어서 길이를 제한하기 위해 max_length도 조절했습니다. 뿐만 아니라 단어가 연속적으로 중복되는 경우 똑같이 번역되는 것을 방지하기 위해서 ngram 기법을 사용했습니다.

이후 streamlit을 통해 demo앱을 만들었습니다. 앱은 BGM, 가사 입력(input), 입력언어-출력언어 설정 selectbox, 번역된 가사 출력(output), 번역된 가사를 음성출력으로 구성했습니다.
아쉬웠던 점은 동원할 수 있는 자원이 더 많았다면, 시간이 더 충분했다면 더 정확도가 높은 가사 번역기를 만들 수 있었을 것 같다는 점과 streamlit을 배우고 만들어 본 것은 좋았지만 어플이나 사이트로 만들어 볼 수 있었을 거라고 생각합니다. 또, 저희 팀은 m2m100을 이용했기 때문에 영어-한국어뿐만 아니라 다양한 언어도 추가할 수 있었을 것 같습니다. 다국적 가사 번역기로도 발전시킬 수 있었다고 생각합니다.


