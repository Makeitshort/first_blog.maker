---
layout: post
title:  "밑바닥부터 시작하는 딥러닝1(3)"
date:   2022-07-01
author: 이준엽
categories: 개인공부
tags:	자연어처리
cover:  "/assets/instacode.png"
---

# Notice

밑바닥부터 시작하는 딥러닝<사이토 고키>, <개앞맵시>, <한빛미디어>를 바탕으로 공부한 내용임을 밝힙니다. 챕터2부터 시작합니다.

# 3.1 퍼셉트론에서 신경망으로

신경망은 **‘입력층’, ‘출력층’, ’은닉층’**으로 나뉘어져있다. 은닉층의 뉴런은 입력층과 출력층과는 달리 눈에 보이지 않기 때문에 ‘은닉’이라고 한다. 신경망은 퍼셉트론과 굉장히 비슷한 구조다. 퍼셉트론을 하나의 식으로 나타내면 $$y = h(b+w_1x_1+w_2x_2)$$로 나타낼 수 있다.

$$h(x)=\begin{cases}0(x\le\ 0)\\1(x>0)\end{cases}$$

즉, 입력신호의 총합이 $$h(x)$$라는 함수를 거쳐 변환되어, 그 변환된 값이 $$y$$의 출력이 되는 것을 보여준다. 이처럼 입력신호의 총합을 출력 신호로 변환하는 함수는 일반적으로 ‘**활성화 함수(Activation Function)’**라 합니다. ‘활성화’라는 이름이 말해주듯, 활성화 함수는 입력신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. 

# 3.2 활성화 함수

활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 ‘**계단 함수’**라고 한다. 즉, 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있다.

계단 함수는 위의 식처럼 입력이 0을 넘으면 1을 출력하고, 그외에는 0을 출력하는 함수다. 이러한 계단 함수를 파이썬으로 단순하게 구현하면 다음과 같다.

```python
def step_function(x):
	y = x > 0
	return y.astypr(np.int)
```

‘**시그모이드 함수(Sigmoid function)’**는 신경망에서 자주 이용하는 활성화 함수이다. 시그모이드 함수 를 식으로 나타내면 다음과 같다.

$$
h(x) = {1 \over 1+ \exp(-x)}
$$

$$\exp(-x)$$는 $$e^{-x}$$를 뜻하며, $$e$$는 자연상수로 2.7182… 의 값을 갖는 실수다. 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. 

시그모이드 함수도 파이썬으로 구현할 수 있다.

```python
def sigmoid(x):
		return 1 / (1 + np.exp(-x))
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5e4d827-522e-4713-9e57-bd1e2ec90785/Untitled.png)

시그모이드 함수와 계단 함수를 비교해보면 가장 먼저 느껴지는 차이점은 매끄러움 일 것이다. 시그모이드 함수(실선)는 부드러운 곡선이다. 반면에 계단 함수(점선)는 입력 0을 경계로 출력이 급작스럽게 바뀐다. 즉, 계단 함수가 0과 1중 하나의 값만 돌려주는 반면 시그모이드 함수는 실수를 돌려준다. 시그모이드 함수의 이 매끈함은 신경망 학습에서 아주 중요한 역할을 한다. 

두 함수는 공통점도 가진다. 큰 관점에서 보면 둘은 같은 모양을 가진다. 둘 다 입력이 작을 때의 출력은 0에 가깝고, 입력이 커지면 출력이 1에 가까워지는 구조다. 출력이 0에서 1사이라는 것도 둘의 공통점이다.

두 함수 모두 ‘**비선형 함수**’라는 것도 공통점이다. 비선형 함수는 문자 그대로 ‘선형이 아닌’ 함수다. 즉, 직선 1개로는 그릴 수 없는 함수다. 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. 선형 함수의 문제는 층을 아무리 깊게 해도 ‘은닉층이 없는 네트워크’로도 똑같은 기능을 할 수 있다는 데 있다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 한다.

계단 함수, 시그모이드 외에도 활성화 함수는 여러 가지가 있다. 그 중 하나는 **ReLU(Rectified Linear Unit)함수**다. ReLU는 입력이 0 을 넘으면 그 입력을 그대로 출력하고, 0이하면 0을 출력하는 함수다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/25273107-d097-4671-a115-59797d65012a/Untitled.png)

수식으로는 다음과 같이 쓸 수 있다. 

$$
h(x)=
\begin{cases}
x(x > 0)\\0(x\le0)
\end{cases}
$$

ReLu 함수는 파이썬으로도 간단하게 구현할 수 있다. 

```python
def relu(x):
		return np.maximum(0,x)
```

# 3.4 3층 신경망 구현하기

3층 신경망의 입력층에서 ‘1층의 첫 번째 뉴런’으로 가는 신호를 보자. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d3d590e0-3ac2-4f97-a1f1-cb5b63245b84/Untitled.png)

편향을 뜻하는 뉴런인 1이 추가됐다. 1층의 $$a_1^{(1)}$$을 수식으로 나타내면 $$a_1^{(1)}$$은 가중치를 곱한 신호 두 개와 편향을 합해서 계산한다. 

$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{(12)}^{(1)}x_2 + b_1^{(1)}
$$

여기에서 행렬의 곱을 이용하면 1층의 ‘가중치 부분’을 다음식처럼 간소화할 수 있다.

$$
A^{(1)} = XW^{(1)} + B^{(1)}
$$

이때 행렬 $A^{(1)}, X,B^{(1)},W^{(1)}$은 다음과 같다.

$$
A^{(1)} = \begin{pmatrix} a^{(1)}_1 \ a^{(1)}_2\ a^{(1)}_3\end{pmatrix} \ X^{(1)} = \begin{pmatrix} x^{(1)}_1 \ x^{(1)}_2\ \end{pmatrix}\ B^{(1)} = \begin{pmatrix} b^{(1)}_1 \ b^{(1)}_2\ b^{(1)}_3\end{pmatrix}\\W^{(1)} = \begin{pmatrix} w^{(1)}_{11} \ w^{(1)}_{21}\ w^{(1)}_{31} \\w^{(1)}_{12} \ w^{(1)}_{22}\ w^{(1)}_{32} \end{pmatrix} 
$$

1층을 파이썬으로 구현해보자.

```python
X = np.array([1.0,0.5])
W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
B1 = np.array([0.1,0.2,0.3])

A1 = np.dot(X, W1) +B1
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c7dd43ce-3194-4af8-9808-baf144369681/Untitled.png)

위 그림은 은닉층을 표현한다. 가중치의 합(가중 신호와 편향의 총합)을 a로 표기하고 활성화 함수 h()로 변환 된 신호를 z로 펴기한다. 여기에서는 활성화 함수로 시그모이드 함수를 사용하기로 한다. 이를 파이썬으로 구현하면 다음과 같다. 

```python
Z1 = sigmoid(A1)
```

여기서 sigmoid함수는 [앞에서](https://www.notion.so/1-3-5001f262ed934f079bd21e158e267053) 정의한 함수다. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c76c1ece-36b5-41bc-a6b8-155863e681b9/Untitled.png)

이어서 1층에서 2층으로 가는 과정이다. 

```python
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1,0.2])

A2 = np.dot(Z1,W2) + B2
Z2 = sigmoid(A2)
```

1층의 출력인 Z1이 2층의 입력이 된다는 점을 제외하면 1층과 구현이 같다. 마지막으로 2층에서 출력층까지의 신호전달도 구현해보자. 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9841eb32-d5d8-4936-8375-0adbf8ce8445/Untitled.png)

```python
def identity_function(x):
		return x

W3 = np.array([[0.1,0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

A3 = np.dot(Z2,W3) + B3
Y = identity_function(A3)
```

여기서 identity_function()은 항등 함수로, 출력층의 활성화 함수로 이용했다. 항등함수는 입력을 그대로 출력하는 함수다. 

지금까지 구현들을 정리하면 다음과 같다.

```python
def init_network():
		network = {}
		network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
		network['B1'] = np.array([0.1,0.2,0.3])
		network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
		network['B2'] = np.array([0.1,0.2])
		network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
		network['B3'] = np.array([0.1,0.2])

		return network

def forward(network, x):
		W1, W2, W3 = network['W1'], network['W2'], network['W3']
		B1, B2, B3 = network['B1'], network['B2'], network['B3']

		A1 = np.dot(X, W1) + B1
		Z1 = sigmoid(A1)
		A2 = np.dot(Z1,W2) + B2
		Z2 = sigmoid(A2)
		A3 = np.dot(Z2,W3) + B3
		y = identity_function(A3)
		
		return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y) #[0.31682708  0.69627909] 
```

여기서 init_network()와 forward()라는 함수를 정의했다. init_network()함수는 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 network에 저장한다. 그리고 network에는 각 층에 필요한 매개변수들이 저장된다. 그리고 forward() 함수는 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현하고 있다. 

# 3.5 출력층 설계하기